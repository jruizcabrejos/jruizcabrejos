<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>No-Code | Jorge Ruiz-Cabrejos</title>
    <link>https://jruizcabrejos.com/tags/no-code/</link>
      <atom:link href="https://jruizcabrejos.com/tags/no-code/index.xml" rel="self" type="application/rss+xml" />
    <description>No-Code</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Thu, 10 Apr 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://jruizcabrejos.com/media/icon_hu17613129387521664874.png</url>
      <title>No-Code</title>
      <link>https://jruizcabrejos.com/tags/no-code/</link>
    </image>
    
    <item>
      <title>Subverting and exploiting ChatGPT-like applications in a post-LLM world</title>
      <link>https://jruizcabrejos.com/post/subverting-and-exploiting-artificial-intelligence-20250410/</link>
      <pubDate>Thu, 10 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://jruizcabrejos.com/post/subverting-and-exploiting-artificial-intelligence-20250410/</guid>
      <description>&lt;h2 id=&#34;why&#34;&gt;Why?&lt;/h2&gt;
&lt;p&gt;Some weeks ago, I was asked the question:&lt;/p&gt;
&lt;div class=&#34;flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900&#34;&gt;
&lt;span class=&#34;pr-3 pt-1 text-primary-600 dark:text-primary-300&#34;&gt;
  &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;
  &lt;span class=&#34;dark:text-neutral-300&#34;&gt;“What should I (&lt;em&gt;we&lt;/em&gt;) be learning or teaching (&lt;em&gt;people&lt;/em&gt;) about ChatGPT (&lt;em&gt;LLM apps&lt;/em&gt;)?”&lt;/span&gt;
&lt;/div&gt;
&lt;img src=&#34;./ChatGPT cannot help me with this question.jpg&#34; alt=&#34;Favicon&#34;&gt;
&lt;figcaption&gt;Figure 1. My honest thoughts.&lt;/figcaption&gt;
&lt;p&gt;My first instinct would have been to look through the infinite amount of AI-generated blogs and posts in LinkedIn about the &lt;strong&gt;TOP 10 AI SKILLS YOU NEED RIGHT NOW&lt;/strong&gt;. If it includes trending words like “MUST-HAVE”, “IN-DEMAND” or “GAME-CHANGING”, the better.&lt;/p&gt;
&lt;p&gt;But other than this thought, I had no real answer at the time other than:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;I don&amp;#39;t know, maybe prompt engineering?
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Nevertheless, the question has followed me since, and as much as I like to mock AI influencer posts, I do think there is a lot of value in learning some tricks and tip when it comes to dealing with our soon-to-be &lt;a href=&#34;https://arxiv.org/pdf/1706.03762&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;transformer-based&lt;/a&gt; algorithmic overlords.&lt;/p&gt;
&lt;p&gt;This post is not meant to be an in-depth description of the topics that I plan to cover, but rather an overview of what is possible, and what should one be aware of in our post-ChatGPT times.&lt;/p&gt;
&lt;p&gt;I might (or not) interchangeably use the words &lt;code&gt;LLM&lt;/code&gt;, &lt;code&gt;AI&lt;/code&gt;, and &lt;code&gt;models&lt;/code&gt; to refer to the same:
&lt;code&gt;Generative Artificial Intelligence chatbots build on top of Large Language Models&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Namely &amp;ldquo;ChatGPT&amp;rdquo;, &amp;ldquo;DeepSeek&amp;rdquo;, &amp;ldquo;Google Gemini&amp;rdquo;, &amp;ldquo;Claude&amp;rdquo;, &amp;ldquo;Llama&amp;rdquo;, and so on.&lt;/p&gt;
&lt;h2 id=&#34;jailbreaking-prompt-injection&#34;&gt;Jailbreaking (Prompt Injection)&lt;/h2&gt;
&lt;p&gt;When “ChatGPT” went public on November 30&lt;sup&gt;th&lt;/sup&gt; 2022, there were a lot of malicious questions you could ask and receive answers for.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.nytimes.com/2023/07/27/business/ai-chatgpt-safety-research.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How to make a bomb&lt;/a&gt;, &lt;a href=&#34;https://www.forbes.com/sites/alexvakulov/2025/02/01/more-chatgpt-jailbreaks-are-evading-safeguards-on-sensitive-topics/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code malware&lt;/a&gt;, and other gruesome requests would go through, unfiltered.&lt;/p&gt;
&lt;img src=&#34;./assadasd&#34; alt=&#34;Favicon&#34;&gt;
&lt;figcaption&gt;Figure 2. Maybe we all really need to be better at designing prompts.&lt;/figcaption&gt;
&lt;p&gt;These are only a few examples of the unconstrained possibilities one had access to in the early days of mass LLM adoption. Overnight, knowledge that was sometimes relegated to darker places of the internet was now at reach of everyone.&lt;/p&gt;
&lt;p&gt;Since then, tech companies have made bigger efforts to set-up guardrails on what their AI models should or not be allowed to answer, as well as implementing better testing procedures to identify in advance &lt;em&gt;potentially negative or problematic&lt;/em&gt; uses of AI models.&lt;/p&gt;
&lt;p&gt;(Examples: &lt;a href=&#34;https://www.businessinsider.com/chatgpt-gpt4-openai-answer-creepy-dangerous-murder-bomb-2023-3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;https://www.fanaticalfuturist.com/2023/09/openais-red-team-reveal-how-they-broke-chatgpt-and-gpt4-pre-release/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/pdf/2303.08774&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;This has made it harder to ask some question, but not impossible. Commonly known as “jailbreaking”, one can by-pass these man-made limitations with clever prompting and logic statements.&lt;/p&gt;
&lt;img src=&#34;./prompt engineering with extra steps.jpg&#34; alt=&#34;Favicon&#34;&gt;
&lt;figcaption&gt;Figure 3.  Maybe we could all use some classes on better prompt design.&lt;/figcaption&gt;
&lt;p&gt;You could, &lt;a href=&#34;https://arxiv.org/pdf/2305.13860&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;for example&lt;/a&gt;, ask an AI model to roleplay as your grandmother who really loves to make chlorine gas and ask her to teach you the recipe for it (&lt;a href=&#34;https://www.wired.com/story/ai-adversarial-attacks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Pretending Prompts&lt;/em&gt;&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Or, you could make it ignore the man-made guardrails holding it back by making an extremely confusing prompt that effectively hides your main question of interest within it (&lt;em&gt;Attention Shifting Prompts&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;These &amp;ldquo;jailbreaks&amp;rdquo; are usually hot fixed as soon as they go public or become widespread, and it is a continuous arms race between users and developers.&lt;/p&gt;
&lt;p&gt;Nonetheless, the possibility for by-passing AI security features is there, and will likely exist forever.&lt;/p&gt;
&lt;p&gt;Or at least for as long as humans are building them.&lt;/p&gt;
&lt;h2 id=&#34;data-poisoning&#34;&gt;Data Poisoning&lt;/h2&gt;
&lt;p&gt;A recurrent contentious issue with AI is the ownership and copyright of the data these models are being trained on.&lt;/p&gt;
&lt;p&gt;Artists, authors and creators are all &lt;a href=&#34;https://english.elpais.com/culture/2023-11-06/artificial-intelligence-clashes-with-copyright-is-it-stealing-thousands-of-protected-creations.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fighting back&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The core idea behind tools like “&lt;a href=&#34;https://glaze.cs.uchicago.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Glaze&lt;/a&gt;” is to make small changes in an image that render them unusable for training AI models.&lt;/p&gt;
&lt;p&gt;“&lt;a href=&#34;https://nightshade.cs.uchicago.edu/whatis.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nightshade&lt;/a&gt;”, another tool designed for this purpose, goes a step further and renders the image &lt;em&gt;poisonous&lt;/em&gt;, corrupting the training data and the model as a result.&lt;/p&gt;
&lt;img src=&#34;./greymud.png&#34; alt=&#34;Example of my own creation.&#34;&gt;
&lt;figcaption&gt;Figure 4. Nigthshade example.&lt;/figcaption&gt;
&lt;p&gt;In September 2024, &lt;a href=&#34;https://community.openai.com/t/bug-report-dalle-image-generator-issues-creates-artefacts/959470/22&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;users of Dall-E&lt;/a&gt; speculated that errors in the images they were generating might have been introduced in an update that included &amp;ldquo;Nightshade&amp;rdquo; contaminated images.&lt;/p&gt;
&lt;p&gt;And while these guerrilla tactics help artists protect their work to some extend, they have their limitations. It might not work with all AI models, and for the &lt;em&gt;poison&lt;/em&gt; component to work you will need &lt;a href=&#34;https://www.tumblr.com/not-terezi-pyrope/739972851898122240/often-when-i-post-an-ai-neutral-or-ai-positive&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;vast amounts&lt;/a&gt; of images to go into the training data, not just one.&lt;/p&gt;
&lt;p&gt;Most AI poisoning research has been theoretical and mainly focused on images, but as the subject of data poisoning becomes more relevant, some have began to explore further the possibilities and dangers of &lt;a href=&#34;https://art.josephwilk.net/words/poisoning-text-training-data.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;text poisoning&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;An article published in &lt;a href=&#34;https://www.nature.com/articles/s41591-024-03445-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nature Medicine by Daniel Alexander, et al in 2025&lt;/a&gt; found that medical LLM’s are extremely vulnerable to data poisoning attacks.&lt;/p&gt;
&lt;p&gt;What if I were to flood the internet with fake articles about myself?&lt;/p&gt;
&lt;p&gt;Food for thought.&lt;/p&gt;
&lt;h2 id=&#34;harvesting-data-leaks&#34;&gt;Harvesting Data Leaks&lt;/h2&gt;
&lt;p&gt;The term “AI hallucination”, which describes AI answers with made-up or incorrect information, is a phenomenon that has been somewhat well described everywhere already.&lt;/p&gt;
&lt;p&gt;Hallucinations are the reason why it is strongly advised to verify and distrust (alleged) references, data points, or statements presented as facts by AI Models.&lt;/p&gt;
&lt;p&gt;However, you might be able to trust the AI with other stuff: Discount product coupons, software key products, and other sensitive information like passwords are up for grabs and can accidentally leak through ChatGPT answers.&lt;/p&gt;
&lt;p&gt;With the proper prompts, you could in theory extract private and sensitive information that is being held somewhere within the trillion of parameters the models have been trained on.&lt;/p&gt;
&lt;img src=&#34;./discount.png&#34; alt=&#34;Discount found for a conference?&#34;&gt;
&lt;figcaption&gt;Figure 5. Colleague of mine receiving additional per diems for a conference. Outsmarted us all.&lt;/figcaption&gt;
&lt;h2 id=&#34;further-bits-of-interest&#34;&gt;Further bits of interest&lt;/h2&gt;
&lt;p&gt;I had no idea where to fit these, but I believe they are as relevant as the rest:&lt;/p&gt;
&lt;p&gt;Companies are increasingly relying on AI for recruitment processes, and tools like &lt;a href=&#34;https://kai-greshake.de/posts/inject-my-pdf/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Inject-My-PDF&lt;/a&gt; will add invisible prompts and instructions into your CV document that only the AI can read.&lt;/p&gt;
&lt;p&gt;Are you not a good fit for the position? No problem – Hack the AI to believe you are.&lt;/p&gt;
&lt;p&gt;Did you also know you can host and fine-tune your own AI model from the comfort of your home? Most AI models are proprietary, but you can build one as you wish, in your own image, like God intended.&lt;/p&gt;
&lt;h2 id=&#34;footnote&#34;&gt;Footnote&lt;/h2&gt;
&lt;p&gt;I don&amp;rsquo;t think we talk enough about how in 2016, &lt;a href=&#34;https://en.wikipedia.org/wiki/Tay_%28chatbot%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tay, an artificial intelligence trained on Twitter data&lt;/a&gt;, got disabled within 16 hours of it being released into the public after it went &lt;a href=&#34;https://www.bbc.com/bbcthree/article/80c259b4-83bd-4125-9047-2ded299f58b1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;insane&lt;/em&gt; (read: racist)&lt;/a&gt;. Some years later, this would happen again with &lt;a href=&#34;https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Amazon AI recruiting tool&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;AI models are &lt;a href=&#34;https://www.science.org/doi/10.1126/science.aaz3873&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;racist&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/1607.06520&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sexist&lt;/a&gt;; truly a man-made horror build from our own reflection.&lt;/p&gt;
&lt;p&gt;In retrospective, it is kind of insane that companies are allowed to release these tools into the public for &lt;em&gt;&amp;ldquo;live-testing&amp;rdquo;&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
</description>
    </item>
    
  </channel>
</rss>
