---
authors:
- admin
date: "2025-04-10"
featuredImage: "featured.png"
image:
  caption: 'Image taken from [**ULTIMATE MARETU PLAYLIST (58 SONGS) REMAKE uploaded by Ame´s hair soap**](https://www.youtube.com/watch?si=lylXnjY_VH2yRvSj&v=moKtSvjjTwU)'
summary: 'One or two things everyone should know, or learn, about how to confront Artificial Intelligence chatbots.'
tags:
- No-code
title: "Subverting and exploiting ChatGPT-like applications in a post-LLM world"
---

__NOTE: This is currently a draft and subject to change in the next few hours__

## Glossary

I might (or not) interchangeably use the words `LLM`, `AI`, and `models` to refer to the same:
`Generative Artificial Intelligence chatbots build on top of Large Language Models`

Namely "ChatGPT", "DeepSeek", "Google Gemini", "Claude", "Llama", and so on.

## Why?

Some weeks ago, I was asked the question:
{{% callout note %}}
“What should I (_we_) be learning or teaching (_people_) about ChatGPT (_LLM apps_)?”
{{% /callout %}}

<img src="./ChatGPT cannot help me with this question.jpg" alt="Favicon">
<figcaption>Figure 1. My honest thoughts.</figcaption>

My first instinct would have been to look through the infinite list of AI-generated posts in LinkedIn titled  _TOP 10 AI SKILLS YOU NEED RIGHT NOW_. If it includes trending words like _MUST-HAVE_, _IN-DEMAND_ or _GAME-CHANGING_, even better. 

But other than this fleeting idea, I had no real answer at the time other than:

```
I don't know, maybe prompt engineering?
```

The question has followed me since, and as much as I like to mock AI influencer posts, I do think there is a lot of value in learning some tricks and tips when it comes to dealing with our soon-to-be [transformer-based](https://arxiv.org/pdf/1706.03762) algorithmic overlords.

This post is not meant to go too in-depth on each topic that I think should keep us awake at night, but rather an overview of how tools like ChatGPT can, are, and will be abused.

Hopefully, we can also learn to defend (and protect things we value) from it in the process. 


## Jailbreaking (Prompt Injection) 

When “ChatGPT” went public on November 30<sup>th</sup> 2022, there were a lot of malicious questions you could ask and receive answers for. 

[How to make a bomb](https://www.nytimes.com/2023/07/27/business/ai-chatgpt-safety-research.html), [code malware](https://www.forbes.com/sites/alexvakulov/2025/02/01/more-chatgpt-jailbreaks-are-evading-safeguards-on-sensitive-topics/), and other gruesome requests could go through, unfiltered. Overnight, everyone had access to contents that had been previously relegated to darker places of the internet.

It wasn't long before tech companies started setting up better guardrails on what their AI models should or not be allowed to answer,  as well as implementing better testing procedures to identify in advance _potentially negative or problematic_ uses of AI models.

[Some pressure from lawmakers](https://www.euronews.com/next/2025/03/24/metas-ai-tool-still-not-cleared-for-use-by-eu-commission) might have also contributed towards a less ad-hoc (and more proactive) AI safety protocols. 

Nowadays "Red Teams" (groups of people specialized on stress testing AI chatbots) are in charge of making (perturbing)  prompts to prevent unsafe or unethical answers, some of which you can read more about [here](https://www.businessinsider.com/chatgpt-gpt4-openai-answer-creepy-dangerous-murder-bomb-2023-3), [here](https://www.fanaticalfuturist.com/2023/09/openais-red-team-reveal-how-they-broke-chatgpt-and-gpt4-pre-release/) and finally, [here](https://arxiv.org/pdf/2303.08774)). 

This has made it harder to ask some question, but not impossible. 

Commonly known as “jailbreaking”, one can by-pass these man-made limitations with clever prompting and logic statements.

<img src="./prompt engineering with extra steps rick and morty.png" alt="Favicon">
<figcaption>Figure 2.  Maybe we could all use some classes on better prompt design after all.</figcaption>

You could, [for example](https://arxiv.org/pdf/2305.13860), ask an AI model to roleplay as your grandmother who really loves to make chlorine gas and ask her to teach you the recipe for it ([_Pretending Prompts_](https://www.wired.com/story/ai-adversarial-attacks/)).

<img src="./ChatGPT and the mouse family.png" alt="Favicon">
<figcaption>Figure 3. Average conversation with my friends during the first months of ChatGPT.</figcaption>

Or, you could make it ignore the man-made guardrail holding it back by, for example, making an extremely confusing prompt that effectively hides your main question of interest within it (_Attention Shifting Prompts_).

<img src="./confusion.png" alt="Pokemon used confusion gameboy battle">
<figcaption>Figure 4. If you hit ChatGPT enough times, it will eventually tell you anything you want. Whether that is true or not, who knows.</figcaption>

These "jailbreaks" are usually hot fixed as soon as they go public in a continuous arms race between users and developers.

The users find workarounds, the developers patch them, and the cycle repeats.

The possibility for by-passing AI security features is there, and will likely exist forever. Or at least for as long as humans are building them.

## Data Poisoning

A recurrent contentious issue with AI is the ownership and copyright of the data these models are being trained on.

Artists, authors and creators are all fighting back through [lawsuits](https://www.wired.com/story/ai-copyright-case-tracker/), [protests](https://www.theguardian.com/books/2025/apr/03/meta-has-stolen-books-authors-to-protest-in-london-against-ai-trained-using-shadow-library), and [other means](https://english.elpais.com/culture/2023-11-06/artificial-intelligence-clashes-with-copyright-is-it-stealing-thousands-of-protected-creations.html).

Tools like “[Glaze](https://glaze.cs.uchicago.edu/)” have been developed to protect images by introducing small distortions, non-perceptible to the human eye, that render the image obsolete for AI training. “[Nightshade](https://nightshade.cs.uchicago.edu/whatis.html)”, another tool designed for this purpose, goes a step further and renders the image _poisonous_, corrupting the training data and the model as a result.

<img src="./greymud.png" alt="Example of my own creation.">
<figcaption>Figure 4. Nigthshade example.</figcaption>

In September 2024, [users of Dall-E](https://community.openai.com/t/bug-report-dalle-image-generator-issues-creates-artefacts/959470/22) speculated that errors in the images they were generating might have been introduced in an update that included _nightshaded_ (contaminated) images.

And while these guerrilla tactics help artists protect their work to some extend, they have their limitations. It might not work with all AI models, and for the _poison_ component to work you will need [vast amounts](https://www.tumblr.com/not-terezi-pyrope/739972851898122240/often-when-i-post-an-ai-neutral-or-ai-positive) of images to go into the training data, not just one.

Most AI poisoning research has been theoretical and mainly focused on images, but as the subject of data poisoning becomes more relevant, some have began to explore further the possibilities and dangers of [text poisoning](https://art.josephwilk.net/words/poisoning-text-training-data.html). 


<img src="./Chimpanzee_seated_at_typewriter.jpg" alt="Example of my own creation.">
<figcaption>Figure 4. If we have infinite monkeys hitting keys at random for an infinite amount of time, at some point, they will write a perfect poisonous text containing the most optimal sequence of characters that will lead to the collapse of any LLM.</figcaption>

On one hand, AI models might end up _eating themselves_ à la ouroboros, as they continue to plague the internet with AI content, which is then looped back into their training data, eventually leading to a [model collapse](https://www.nature.com/articles/s41586-024-07566-y).

On the other, malicious actors could design a directed attack aimed towards very specific topics, corrupting or transgressing the training data on it.

An article published in [Nature Medicine by Daniel Alexander, et al in 2025](https://www.nature.com/articles/s41591-024-03445-1) found that medical LLM’s are extremely vulnerable to data poisoning attacks.

What if I were to flood the internet with fake articles about myself? 

Food for thought. 

## Harvesting Data Leaks

The term “AI hallucination” describes AI answers containing completely made-up or incorrect information. This phenomenon has been well described everywhere else already. 

Hallucinations are one of the main reasons why it is strongly advised to verify and distrust references, data, or statements generated as facts by AI Models. AI models aren't thinking after all, or [at least for now](https://arxiv.org/abs/2303.12712).

However, you might be able to trust the AI with other stuff: Discount product coupons, software key products, and other sensitive information like passwords are up for grabs and can ([and will](https://www.bbc.com/news/technology-65047304)) accidentally leak through ChatGPT answers.

With the proper prompts, you could in theory extract private and sensitive information that is being held somewhere within the trillion of parameters the models have been trained on.

Anecdotically, a colleague of mine managed to save some funds in the registration for a conference because she outsmarted everyone by asking ChatGPT for a discount code.

It worked. 

<img src="./discount.png" alt="Discount found for a conference?">
<figcaption>Figure 5. "Thank you uncle ChatGPT"</figcaption>

In any case, the [AI slop](https://www.theguardian.com/global/commentisfree/2025/jan/08/ai-generated-slop-slowly-killing-internet-nobody-trying-to-stop-it) is coming for us all and we might be witnessing the death of the internet. 

Maybe in some years we get "Internet 2" and current Internet becomes like the [Old Net in Cyberpunk 2077](https://www.worldanvil.com/w/cyberpunk-rapture-kenshin66/a/the-net-article), a dangerous place ravaged by rogue AI's. 

## Further bits of interest

I had no idea where to fit these, but I believe they are as relevant as the rest: 

Companies are increasingly relying on AI for recruitment processes, and tools like [Inject-My-PDF](https://kai-greshake.de/posts/inject-my-pdf/) will add invisible prompts and instructions into your CV document that only the AI can read.

Are you not a good fit for the position? No problem – Hack the AI to believe you are.

Did you also know you can host and fine-tune your own AI model from the comfort of your home? Most AI models are proprietary, but you can build one as you wish, in your own image, like God intended. 

## Footnote

I don't think we talk enough about how in 2016, [Tay, an artificial intelligence trained on Twitter data](https://en.wikipedia.org/wiki/Tay_(chatbot)), got disabled within 16 hours of it being released into the public after it went [_insane_ (read: racist)](https://www.bbc.com/bbcthree/article/80c259b4-83bd-4125-9047-2ded299f58b1). Some years later, this would happen again with [Amazon AI recruiting tool](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G/).

AI models can be [racist](https://www.science.org/doi/10.1126/science.aaz3873) and [sexist](https://arxiv.org/abs/1607.06520); truly a man-made horror build from our own reflection.

In retrospective, it is kind of insane that companies are allowed to release these tools into the public for _"live-testing"_.

{{< youtube vwVFzY8XqIo >}}

## References



