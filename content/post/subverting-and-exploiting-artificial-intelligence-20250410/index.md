---
authors:
- admin
date: "2025-04-10"
featuredImage: "featured.png"
image:
  caption: 'Image taken from [**ULTIMATE MARETU PLAYLIST (58 SONGS) REMAKE uploaded by Ame´s hair soap**](https://www.youtube.com/watch?si=lylXnjY_VH2yRvSj&v=moKtSvjjTwU)'
summary: 'One or two things everyone should know, or learn, about how to confront Artificial Intelligence chatbots.'
tags:
- No-code
title: "Subverting and exploiting ChatGPT-like applications in a post-LLM world"
---

WARNING: IF YOU ARE READING THIS, YOU ARE READING A DRAFT :)

Minor changes are likely to happen in the next hours.

## Glossary

I might (or not) interchangeably use the words `LLM`, `AI`, and `models` to refer to the same:
`Generative Artificial Intelligence chatbots build on top of Large Language Models`

Namely "ChatGPT", "DeepSeek", "Google Gemini", "Claude", "Llama", and so on.

## Why?

Some weeks ago, I was asked the question:
{{% callout note %}}
“What should I (_we_) be learning or teaching (_people_) about ChatGPT (_LLM apps_)?”
{{% /callout %}}

<img src="./ChatGPT cannot help me with this question.jpg" alt="Favicon">
<figcaption>Figure 1. My honest thoughts.</figcaption>

My first instinct would have been to look through the infinite amount of AI-generated blogs and posts in LinkedIn about the **TOP 10 AI SKILLS YOU NEED RIGHT NOW**. If it includes trending words like “MUST-HAVE”, “IN-DEMAND” or “GAME-CHANGING”, the better. 

But other than this fleeting idea, I had no real answer at the time other than:

```
I don't know, maybe prompt engineering?
```

Nevertheless, the question has followed me since, and as much as I like to mock AI influencer posts, I do think there is a lot of value in learning some tricks and tips when it comes to dealing with our soon-to-be [transformer-based](https://arxiv.org/pdf/1706.03762) algorithmic overlords.

This post is not meant to be an in-depth description of topics I think are relevant and everyone should be aware of, but rather an overview of how tools like ChatGPT can, are, and will be abused.

Hopefully, we can also learn to defend (and protect things) from it. 


## Jailbreaking (Prompt Injection) 

When “ChatGPT” went public on November 30<sup>th</sup> 2022, there were a lot of malicious questions you could ask and receive answers for. 

[How to make a bomb](https://www.nytimes.com/2023/07/27/business/ai-chatgpt-safety-research.html), [code malware](https://www.forbes.com/sites/alexvakulov/2025/02/01/more-chatgpt-jailbreaks-are-evading-safeguards-on-sensitive-topics/), and other gruesome requests could go through, unfiltered. Overnight, knowledge that had been relegated to darker places of the internet was now at reach of everyone.

Since then, tech companies have made bigger efforts to set-up guardrails on what their AI models should or not be allowed to answer, as well as implementing better testing procedures to identify in advance _potentially negative or problematic_ uses of AI models.

(Examples: [here](https://www.businessinsider.com/chatgpt-gpt4-openai-answer-creepy-dangerous-murder-bomb-2023-3), [here](https://www.fanaticalfuturist.com/2023/09/openais-red-team-reveal-how-they-broke-chatgpt-and-gpt4-pre-release/) and [here](https://arxiv.org/pdf/2303.08774)). 

This has made it harder to ask some question, but not impossible. 

Commonly known as “jailbreaking”, one can by-pass these man-made limitations with clever prompting and logic statements.

<img src="./prompt engineering with extra steps rick and morty.png" alt="Favicon">
<figcaption>Figure 2.  Maybe we could all use some classes on better prompt design after all.</figcaption>

You could, [for example](https://arxiv.org/pdf/2305.13860), ask an AI model to roleplay as your grandmother who really loves to make chlorine gas and ask her to teach you the recipe for it ([_Pretending Prompts_](https://www.wired.com/story/ai-adversarial-attacks/)).

<img src="./ChatGPT and the mouse family.png" alt="Favicon">
<figcaption>Figure 3. Average conversation with my friends during the first months of ChatGPT.</figcaption>

Or, you could make it ignore the man-made guardrails holding it back by, for example, making an extremely confusing prompt that effectively hides your main question of interest within it (_Attention Shifting Prompts_).

<img src="./confusion.png" alt="Favicon">
<figcaption>Figure 4. If you hit ChatGPT enough times, it will eventually tell you anything you want. Whether that is true or not, who knows.</figcaption>

These "jailbreaks" are usually hot fixed as soon as they go public or become widespread, and it is a continuous arms race between users and developers.

users find exploits, developers patch them.

Nonetheless, the possibility for by-passing AI security features is there, and will likely exist forever.

Or at least for as long as humans are building them.

## Data Poisoning

A recurrent contentious issue with AI is the ownership and copyright of the data these models are being trained on.

Artists, authors and creators are all [fighting back](https://english.elpais.com/culture/2023-11-06/artificial-intelligence-clashes-with-copyright-is-it-stealing-thousands-of-protected-creations.html) through lawsuits, protests, and other means.

Tools like “[Glaze](https://glaze.cs.uchicago.edu/)” have been developed to protect images by introducing small distortions, non-perceptible to the human eye, that render the image obsolete for AI training. “[Nightshade](https://nightshade.cs.uchicago.edu/whatis.html)”, another tool designed for this purpose, goes a step further and renders the image _poisonous_, corrupting the training data and the model as a result.

<img src="./greymud.png" alt="Example of my own creation.">
<figcaption>Figure 4. Nigthshade example.</figcaption>

In September 2024, [users of Dall-E](https://community.openai.com/t/bug-report-dalle-image-generator-issues-creates-artefacts/959470/22) speculated that errors in the images they were generating might have been introduced in an update that included _nightshaded_ (contaminated) images.

And while these guerrilla tactics help artists protect their work to some extend, they have their limitations. It might not work with all AI models, and for the _poison_ component to work you will need [vast amounts](https://www.tumblr.com/not-terezi-pyrope/739972851898122240/often-when-i-post-an-ai-neutral-or-ai-positive) of images to go into the training data, not just one.

Most AI poisoning research has been theoretical and mainly focused on images, but as the subject of data poisoning becomes more relevant, some have began to explore further the possibilities and dangers of [text poisoning](https://art.josephwilk.net/words/poisoning-text-training-data.html). 

An article published in [Nature Medicine by Daniel Alexander, et al in 2025](https://www.nature.com/articles/s41591-024-03445-1) found that medical LLM’s are extremely vulnerable to data poisoning attacks.

<img src="./Chimpanzee_seated_at_typewriter.jpg" alt="Example of my own creation.">
<figcaption>Figure 4. If we have infinite monkeys hitting keys at random for an infinite amount of time, at some point, they will write a perfect poisonous text containing the most optimal sequence of characters that will lead to the of any LLM.</figcaption>

On one hand, the ai might eat itself and lead to total model collaps as bfjrjf shows in 202x paper
On the other side, you could target specific topics, like health and medicine, for a directed attack. As shown by ....

What if I were to flood the internet with fake articles about myself? 

Food for thought. 

## Harvesting Data Leaks

The term “AI hallucination” describes AI answers containing completely made-up or incorrect information. This phenomenon has been well described everywhere else already. 

Hallucinations are one of the main reasons why it is strongly advised to verify and distrust references, data points, or statements presented as facts by AI Models. AI models aren't thinking after all, or so goes the current debate. (Link sparks of General use art int).

However, you might be able to trust the AI with other stuff: Discount product coupons, software key products, and other sensitive information like passwords are up for grabs and can (and will (CITATION LEAKAGE)) accidentally leak through ChatGPT answers.

With the proper prompts, you could in theory extract private and sensitive information that is being held somewhere within the trillion of parameters the models have been trained on.

Anecdotically, a collegue of mine recieved more funds

<img src="./discount.png" alt="Discount found for a conference?">
<figcaption>Figure 5. "Thank you uncle ChatGPT"</figcaption>

## Further bits of interest

I had no idea where to fit these, but I believe they are as relevant as the rest: 

Companies are increasingly relying on AI for recruitment processes, and tools like [Inject-My-PDF](https://kai-greshake.de/posts/inject-my-pdf/) will add invisible prompts and instructions into your CV document that only the AI can read.

Are you not a good fit for the position? No problem – Hack the AI to believe you are.

Did you also know you can host and fine-tune your own AI model from the comfort of your home? Most AI models are proprietary, but you can build one as you wish, in your own image, like God intended. 

## Footnote

I don't think we talk enough about how in 2016, [Tay, an artificial intelligence trained on Twitter data](https://en.wikipedia.org/wiki/Tay_(chatbot)), got disabled within 16 hours of it being released into the public after it went [_insane_ (read: racist)](https://www.bbc.com/bbcthree/article/80c259b4-83bd-4125-9047-2ded299f58b1). Some years later, this would happen again with [Amazon AI recruiting tool](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G/).

AI models are [racist](https://www.science.org/doi/10.1126/science.aaz3873) and [sexist](https://arxiv.org/abs/1607.06520); truly a man-made horror build from our own reflection.

In retrospective, it is kind of insane that companies are allowed to release these tools into the public for _"live-testing"_.

## References



